#streamlit run Lagnchain_with_bible_250329.py
#ì‹¤í–‰ streamlit run Lagnchain_with_bible_250329.py --server.address=0.0.0.0 --server.port=8501
import streamlit as st

from langchain.schema.runnable import RunnableMap
from langchain.retrievers.multi_query import MultiQueryRetriever

#import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
import os
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain_core.prompts import PromptTemplate

import re

from langchain_google_genai import ChatGoogleGenerativeAI

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever  #TF-IDF ê³„ì—´ì˜ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
from langchain.retrievers import EnsembleRetriever # ì—¬ëŸ¬ retrieverë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì²˜ë¦¬
from langchain_community.vectorstores.faiss import DistanceStrategy #vectorstoresì˜ ê±°ë¦¬ ê³„ì‚°
from langchain.memory import ConversationBufferWindowMemory

import pickle
import time


from google import genai
from google.genai import types
from io import BytesIO
from PIL import Image

from gtts import gTTS
import io

def get_conversation_chain(vectorstore,data_list,query,st_memory):
   
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp-01-21", temperature=0)
    #llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-thinking-exp", temperature=0)
    #llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)

    template = """ë‹¹ì‹ ì€ ì¸ê³µì§€ëŠ¥ ChatBOTìœ¼ë¡œ Question ë‚´ìš©ì— ëŒ€í•´ì„œ ëŒ€ë‹µí•©ë‹ˆë‹¤.
    ëŒ€ë‹µì€ Contextì— ìˆëŠ” ë‚´ìš©ì„ ì°¸ì¡°í•´ì„œë§Œ ë‹µë³€í•˜ê³  ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
    ë˜ë„ë¡ì´ë©´ ìì„¸í•œ ë‚´ìš©ìœ¼ë¡œ ëŒ€ë‹µí•˜ê³  contextì˜ ìˆëŠ” original sourceë„ ê°™ì´ ë³´ì—¬ì£¼ì„¸ìš”.
    #Chat history: 
    {chat_history}
    #Context: 
    {context}
    #Question:
    {question}

    #Answer:
    """

    prompt = ChatPromptTemplate.from_template(template)

    faiss_retriever=vectorstore.as_retriever(search_type="mmr",
    search_kwargs={'k':10, 'fetch_k': 30})

    # initialize the bm25 retriever(10ê°œ)
    bm25_retriever = BM25Retriever.from_documents(data_list)
    bm25_retriever.k = 10

    f_ratio = 0.7
    # initialize the ensemble retriever
    #retriever ê°€ì¤‘ì¹˜ ì„¤ì •(bm25:30% + faiss:70%)
    # ë¬¸ì„œ ê²°í•© ë°©ì‹ ì„¤ì •(default setting:combine_documents-ê²°í•©ëœ ë¬¸ì„œë“¤ì„ í•©ì¹˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘)
    ensemble_retriever_combine = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever], weights=[1-f_ratio, f_ratio] 
        ,retriever_type="combine_documents")
    
    multiqueryretriever = MultiQueryRetriever.from_llm(ensemble_retriever_combine, llm=llm)


    memory = st_memory

    chain = (
      RunnableMap({
        "context": lambda x: multiqueryretriever.invoke(x['question']),
        "question": lambda x: x['question'],
        'chat_history' : lambda x: x['chat_history']
    }) 
    | prompt | llm | StrOutputParser())

    #response = chain.invoke({'question': query,
    #                         'chat_history': memory.load_memory_variables({})['chat_history']})
    
    think_message_placeholder = st.empty() # DeltaGenerator ë°˜í™˜
    
    full_response = '' 
    for chunk in chain.stream({'question': query,
                            'chat_history': memory.load_memory_variables({})['chat_history']}):
        full_response += chunk
        think_message_placeholder.markdown(full_response)
    
    memory.save_context({"input": query}, {"output": full_response})

    return full_response

def make_image(response):
    # API í‚¤ ì„¤ì •
    client = genai.Client(api_key=st.session_state.gemini_api_key)

    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp-image-generation", temperature=0)

    prompt_template = PromptTemplate(
        template="""
        The section content is a story from the Bible. Based on the following section content, create a prompt to generate a Ghibli Studio-style image to represent this section. Your prompt should be less than 300 characters. Write your prompt in English.
        
        Section content:
        
        {section_content}
        
        Prompt:""",
        input_variables=["section_content"],
    )
    
    image_prompt = llm.invoke(prompt_template.format(section_content=response))

    contents = image_prompt.content

    # Gemini 2.0 Flash Experimental ëª¨ë¸ ì‚¬ìš©
    response = client.models.generate_content(
        model="models/gemini-2.0-flash-exp",
        contents=contents,
        config=types.GenerateContentConfig(
            response_modalities=['Text', 'Image'],
            temperature=0.7,
        )
    )

    # ì‘ë‹µ ì²˜ë¦¬
    for part in response.candidates[0].content.parts:
        if part.text is not None:
            print(part.text)
        elif part.inline_data is not None:
            image_bytes = part.inline_data.data
            image = Image.open(BytesIO(part.inline_data.data))
            st.image(image)

def print_response(response):

    #ì‹¤ì‹œê°„ ì¶œë ¥(Stream)

    sentence = ''
    
    if '\n' not in response:
        st.write(response)

    else: 
        for chunk in response:
            #st.write(chunk)
            if chunk in ['\n','\n\n', '\n\n\n']:
                st.write(sentence)
                time.sleep(0.1)
                sentence = ''
            else:
                sentence = sentence + chunk

    # Streamlit ì•±ì— ì´ë¯¸ì§€ í‘œì‹œí•˜ê¸°
    #st.image(image, caption='ì´ë¯¸ì§€ ìº¡ì…˜ ì…ë ¥')

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"Human: {message.content}")
        else:
            st.write(f"AI: {message.content}")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_chat_history_str(chat_history):
    return "\n".join([f"{entry['role'].capitalize()}: {entry['content']}" for entry in chat_history])

def file_read():
    folder_path = "C:\\python\\Bible-Rag\\new_data"
    file_list = os.listdir(folder_path)

    documents = []
    pattern2 = r'[ê°€-í£]+'

    for file_name in file_list:
        loader = TextLoader(folder_path+'\\'+file_name, encoding='utf-8')
        document = loader.load()
        result2 = re.search(pattern2, file_name)
        if result2:
            book_name = result2.group()
        document[0].metadata = {'source':book_name}
        documents.append(document)

    document_list = []
    for index,i in enumerate(documents):
        document_list.append(i[0])

    return document_list

def text_splitter(document_list):
    # 2000ì(overlap=50ë¡œ ì²­í‚¹í•˜ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 3000,chunk_overlap = 50)
    #splitterë¥¼ ì´ìš©í•œ ë¬¸ì„œ ì²­í‚¹
    data = text_splitter.split_documents(document_list)

    return data

def make_vectorstore(data):

    #ì„ë² ë”© ëª¨ë¸
    embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')

    vectorstore = FAISS.from_documents(documents=data, embedding=embeddings)

    st.success(f"ì´ {len(data)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    
    return vectorstore

def load_bible(vector_distance_cal):
    with st.spinner("íŒŒì¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
        
        #ì„ë² ë”© ëª¨ë¸ ë¶ˆë¡œì˜¤ê¸°
        embeddings = HuggingFaceEmbeddings(model_name='jhgan/ko-sroberta-multitask')
        
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ(allow_dangerous_deserialization=True í•„ìš”)
        vectorstore = FAISS.load_local("Rag_data/bible_embed2", 
        embeddings,
        distance_strategy=vector_distance_cal, 
        allow_dangerous_deserialization=True)

        with open("Rag_data/bible_data2.pkl", 'rb') as f:
            data_load = pickle.load(f)
    
    st.success(f"ì´ {len(data_load)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")

    return data_load, vectorstore

def text_to_speech(text, language='ko'):

    text_new = re.sub('[^a-zA-Zê°€-í£0-9.,:()?!]', ' ', text)

    # gTTSë¡œ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
    tts = gTTS(text=text_new, lang=language)
    
    # ë©”ëª¨ë¦¬ì— ì˜¤ë””ì˜¤ ì €ì¥
    audio_buffer = io.BytesIO()
    tts.write_to_fp(audio_buffer)
    audio_buffer.seek(0)

    # ì˜¤ë””ì˜¤ ì¬ìƒ
    st.audio(audio_buffer, format="audio/mp3")
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
    st.download_button(
        label="ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ",
        data=audio_buffer,
        file_name="output.mp3",
        mime="audio/mp3"
    )

def main():

    st.set_page_config(page_title="Lagnchain_with_bible", page_icon=":books:")
    st.title("ğŸ¦œğŸ”— Langchain_with_bible")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "vectorstore" not in st.session_state:
        st.session_state.vectorstore = None
    if "document_list" not in st.session_state:
        st.session_state.document_list = []
    if "vector_option" not in st.session_state:
        st.session_state.vector_option = None
    if "gemini_api_key" not in st.session_state:
        st.session_state.gemini_api_key = None
    if "response" not in st.session_state:
        st.session_state.response = None
    if "voice_option" not in st.session_state:
        st.session_state.voice_option = None

    #ìœˆë„ìš° í¬ê¸° kë¥¼ ì§€ì •í•˜ë©´ ìµœê·¼ kê°œì˜ ëŒ€í™”ë§Œ ê¸°ì–µí•˜ê³  ì´ì „ ëŒ€í™”ëŠ” ì‚­ì œ
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferWindowMemory(memory_key="chat_history", k=4,return_messages=True) 

    with st.sidebar:
        st.session_state.gemini_api_key = st.text_input('Gemini_API_KEYë¥¼ ì…ë ¥í•˜ì„¸ìš”.', key="langchain_search_api_gemini", type="password")
        "[Get an Gemini API key](https://ai.google.dev/)"
        "[How to get Gemini API Key](https://luvris2.tistory.com/880)"

        if (st.session_state.gemini_api_key[0:2] != 'AI') or (len(st.session_state.gemini_api_key) != 39):
            st.warning('ì˜ëª»ëœ key ì…ë ¥', icon='âš ï¸')
        else:
            st.success('ì •ìƒ key ì…ë ¥', icon='ğŸ‘‰')

        if process :=st.button("Process"):
            if (st.session_state.gemini_api_key[0:2] != 'AI') or (len(st.session_state.gemini_api_key) != 39):
                st.error("ì˜ëª»ëœ key ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                st.stop()

        if data_clear :=st.button("ëŒ€í™” í´ë¦¬ì–´"):
            st.session_state.conversation = None
            st.session_state.chat_history = []
            st.session_state.memory = ConversationBufferWindowMemory(memory_key="chat_history", k=4,return_messages=True)
            st.session_state.response = None

        st.session_state.voice_option = st.radio(label='ìŒì„± ìƒì„± Option',
                          options=['ìŒì„± ìƒì„±', 'ìŒì„± ë¯¸ìƒì„±'],
                          index=1  # ê¸°ë³¸ ì„ íƒê°’ì€ 'Banana'
                          )
            
        vector_option = ["EUCLIDEAN_DISTANCE","MAX_INNER_PRODUCT", "DOT_PRODUCT"]

        if vector_option_1 := st.selectbox("Select the vector distance cal method?",
                                           options=vector_option,
                                           index=0):

            st.info(f"You selected: {vector_option_1}")

            if vector_option_1 == "EUCLIDEAN_DISTANCE":
                #ìœ í´ë¦¬ë“œ ê±°ë¦¬(L2)
                st.session_state.vector_option = DistanceStrategy.EUCLIDEAN_DISTANCE 
                
            if vector_option_1 == "MAX_INNER_PRODUCT":
                #ë‚´ì (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ìœ ì‚¬)
                st.session_state.vector_option = DistanceStrategy.MAX_INNER_PRODUCT

            if vector_option_1 == "DOT_PRODUCT":
                #ì ê³±(ë‚´ì ê³¼ ë™ì¼)
                st.session_state.vector_option = DistanceStrategy.DOT_PRODUCT  
            

    #0. gemini api key Setting
    if not st.session_state.gemini_api_key:
        st.warning("Gemini API Keyë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    #genai.configure(api_key=gemini_api_key)

    #0. gemini api key Setting
    os.environ["GOOGLE_API_KEY"] = st.session_state.gemini_api_key


    # íŒŒì¼ì´ ì—…ë¡œë“œë˜ë©´ ì²˜ë¦¬
    if st.session_state.vectorstore == None:

        st.session_state.document_list, st.session_state.vectorstore = load_bible(st.session_state.vector_option)

    st.chat_message("assistant").write("ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

    #2. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥
    # st.session_state['chat_history']ê°€ ìˆìœ¼ë©´ ì‹¤í–‰
    if ("chat_history" in st.session_state) and (len(st.session_state['chat_history'])>0):
        #st.session_state['messages']ëŠ” tuple í˜•íƒœë¡œ ì €ì¥ë˜ì–´ ìˆìŒ.
        for role, message in st.session_state['chat_history']: 
            st.chat_message(role).write(message)

    #3. queryë¥¼ ì…ë ¥ë°›ëŠ”ë‹¤.
    if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):

        #4.'user' iconìœ¼ë¡œ queryë¥¼ ì¶œë ¥í•œë‹¤.
        st.chat_message("user").write(query)
        #5. queryë¥¼ session_state 'user'ì— append í•œë‹¤.
        st.session_state['chat_history'].append(('user',query))
        

        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):

                start_time = time.time()

                # chain í˜¸ì¶œ
                st.session_state.response = get_conversation_chain(
                    st.session_state.vectorstore,
                    st.session_state.document_list,
                    query,
                    st.session_state.memory)
                
                #response ì¶œë ¥
                #print_response(response)

                #ì´ë¯¸ì§€ íŒŒì¼ ë§Œë“¤ê¸°
                make_image(st.session_state.response)

                #st.session_state.response = str(st.session_state.response)

                #ë‹µë³€ ìŒì„± ë“£ê¸°
                if len(st.session_state.response) > 5000:
                    st.warning('ë‹µë³€ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ì„œ ìŒì„± íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
                else:
                    if st.session_state.voice_option == 'ìŒì„± ìƒì„±':
                        text_to_speech(text=st.session_state.response,language='ko')

                end_time = time.time()
                total_time = (end_time - start_time)
                st.info(f"ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {total_time}ì´ˆ")

                #st.write(response)
                #6. response session_state 'assistant'ì— append í•œë‹¤.
                st.session_state['chat_history'].append(('assistant'
                                                         ,st.session_state.response))

if __name__ == '__main__':
    main()